{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683f23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import jieba\n",
    "from zhon import hanzi as zh_hanzi\n",
    "\n",
    "from cedict_utils.cedict import CedictParser\n",
    "\n",
    "from dragonmapper import transcriptions\n",
    "from dragonmapper import hanzi\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import hashlib\n",
    "\n",
    "from sys import platform\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa2f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence():\n",
    "    \n",
    "    def __init__(self, sentence, seg_list, english):\n",
    "        self.sentence = sentence \n",
    "        self.seg_list = seg_list\n",
    "        self.english = english\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \" \".join(self.seg_list) + \"\\n\" + self.english\n",
    "    \n",
    "    def score(self, level=3) -> int:\n",
    "        level_dict = dict()\n",
    "        score = 0\n",
    "        for w in self.seg_list:\n",
    "            w_level = hsk_level_new(w)\n",
    "            level_dict[w] = w_level\n",
    "            if w_level in ['4', '5']:\n",
    "                score += 1\n",
    "            elif w_level in ['6', '7-9']:\n",
    "                score += 2\n",
    "            elif w_level == \"0\":\n",
    "                score += 3\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49765a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = set(zh_hanzi.punctuation)\n",
    "\n",
    "parser = CedictParser()\n",
    "entries = parser.parse()\n",
    "\n",
    "CEDICT = dict()\n",
    "for e in entries:\n",
    "    CEDICT[e.simplified] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47afe6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "HSK = dict()\n",
    "words = set()\n",
    "resource_path = \"/home/daniel/programming/chinese/chinese-tools/Mandarin corner/resources/hsk3.0-\"\n",
    "levels = [str(i) for i in list(range(1,7))] + [\"7-9\"]\n",
    "\n",
    "for level in levels:\n",
    "    file_path = resource_path + level + '.txt'\n",
    "    with open(file_path) as f:\n",
    "        text = f.read()\n",
    "        text_words = text.split(\"\\n\")\n",
    "        text_words = set(text_words)\n",
    "        words.update(text_words)\n",
    "        HSK[level] = words.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8c1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsk_level_new(word: str) -> str:\n",
    "\n",
    "    for i in range(1,7):\n",
    "        if word in HSK[str(i)]:\n",
    "            return str(i)\n",
    "\n",
    "    if word in HSK[\"7-9\"]:\n",
    "        return \"7-9\"\n",
    "    \n",
    "    return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b42b3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.purpleculture.net/sample-sentences/?word=酸痛'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD = \"酸痛\"\n",
    "# WORD = \"还清\"\n",
    "url = \"https://www.purpleculture.net/sample-sentences/?word=\" + WORD\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64ddf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd4badd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b510ad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.403 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "Sentences = list()\n",
    "\n",
    "sample_sentences = soup.findAll('span', attrs={'class':'sc samplesen'})\n",
    "print(\"sentences:\", len(sample_sentences))\n",
    "en_sentences = soup.findAll('div', attrs={'class':'sample_en'})\n",
    "\n",
    "seg_map = dict()\n",
    "\n",
    "for i, sent in enumerate(sample_sentences):\n",
    "    chars = sent.findAll('span', attrs={'class':'cnchar'})\n",
    "    char_sent = \"\".join(char.text for char in chars)\n",
    "#     print(char_sent, en_sentences[i].text)\n",
    "    \n",
    "    seg_list = list(jieba.cut(char_sent, cut_all=False))\n",
    "#     pinyin = \" \".join(hanzi.to_pinyin(word) for word in seg_list)\n",
    "    \n",
    "    seg_map[char_sent] = seg_list\n",
    "#     print(seg_map)\n",
    "#     print(char_sent, seg_list, en_sentences[i].text)\n",
    "    Sentences.append(Sentence(char_sent, seg_list, en_sentences[i].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e31b914",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "看着六岁的儿子在花园扭断那些杜鹃花枝，我的心一下子酸痛起来，这一天就这么开始了。\n",
      "以前他什么也不怕，现在他会找安闲自在：刮风下雨，他都不出车；身上有点酸痛，也一歇就是两三天。\n",
      "请别让我们的眼睛因渴望见到你的面容而酸痛。\n",
      "我的帽子已经歪斜，我的围裙已经脏了，我的脚酸痛。\n",
      "那么研究人员如何解释有些人穿上塑身鞋后出现酸痛或者明显反应的情况呢？\n",
      "如果你每30，40分钟站立休息一会或者舒展一下双脚，这样可以减少肌肉疲劳而产生的酸痛感。\n",
      "不得不承认，我有时很喜欢腿部那种经历高强度锻炼或比赛所引发的肌肉酸痛感。\n",
      "在一段艰苦的体育锻炼或是一段长途跑之后，你也许感觉到全身的肌肉酸痛。\n",
      "对他们来说，这是对死去的两代人的酸痛记忆。\n",
      "与老年人相比，儿童可较经常出现发热、酸痛和头痛。\n",
      "运动过度的另一个指标是肌肉酸痛持续数天不消失。\n",
      "如果在健身之后你太早吃东西的话，你可能最小化肌肉僵硬和酸痛。\n",
      "许多专业跑步者会洗冰浴以减轻跑步后的酸痛。如果你不能忍受冰浴，在酸痛处敷上冰袋。\n",
      "这是因为通过降低运动强度能减少肌肉中导致酸痛和其他不适感觉的乳酸的缘故。\n",
      "任何人一旦出现突然发烧、咳嗽和肌肉酸痛等流感症状，应避免去工作场所或公共交通设施，赶紧去求医。\n",
      "在冬天很难到达该地，但在夏天，当道路上没有什么车辆时，徒步旅行者会在到来后在地的热水中快速浸泡，以缓和他们肌肉的酸痛。这些地表水最后汇聚成细流小溪。\n",
      "头晕、失眠、全身酸痛、口干舌燥……长期使用电脑的人都会遇到这些情况。\n",
      "可是你来我这里抱怨你的膝盖疼，你的背痛的要死，你的脚也酸痛，你还没上了半截楼梯你就喘不上气来了。\n",
      "只拿那些你感兴趣的公司的宣传品，因为你会阅读它们，否则背着一书包各公司的宣传品走来走去会让你胳臂酸痛。\n",
      "叫女佣人拿去给你做茶点：区区一物，你不必客气，因为它又软又滑，喉咙酸痛吃下去也不碍事。\n"
     ]
    }
   ],
   "source": [
    "for i in Sentences:\n",
    "    print(i.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5fd2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentences.sort(key=Sentence.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79729631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('div', attrs={'class' : 'samplesen'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbe1cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那么研究人员如何解释有些人穿上塑身鞋后出现酸痛或者明显反应的情况呢？\n",
      "在冬天很难到达该地，但在夏天，当道路上没有什么车辆时，徒步旅行者会在到来后在地的热水中快速浸泡，以缓和他们肌肉的酸痛。这些地表水最后汇聚成细流小溪。\n"
     ]
    }
   ],
   "source": [
    "print(Sentences[0].sentence)\n",
    "print(Sentences[-1].sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ecce3bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"https://www.purpleculture.net/dictionary-details/?word=还清\"\n",
    "response = requests.get(url2)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "en_sentences = soup.findAll('div', attrs={'class':'sample_en'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "928528fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"sample_en\" id=\"ensen1\">To be honest, I seldom receive others' favor, but this time I really owe you a lot and do not know when I can pay you back. </div>,\n",
       " <div class=\"sample_en\" id=\"ensen2\">Pay off your credit cards every month. </div>,\n",
       " <div class=\"sample_en\" id=\"ensen3\">For example, you need to know about one another' s saving and spending habits like how often (if ever) you balance your checkbooks, pay your credit lines, and research big purchases before you buy. </div>,\n",
       " <div class=\"sample_en\" id=\"ensen4\">If all of these are covered and you still have cash left over (which you will, given some time), the next step is to pay off all of your debts. </div>,\n",
       " <div class=\"sample_en\" id=\"ensen5\">Think about what could change in your life if you paid off that credit card? </div>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0c1902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important data file \"resources/sememe_all\" lost, please run `OpenHowNet.download()`.\n"
     ]
    }
   ],
   "source": [
    "import OpenHowNet\n",
    "hownet_dict = OpenHowNet.HowNetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4318aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resources/resources.zip: 72948KB [00:17, 4246.46KB/s]                                                             \n"
     ]
    }
   ],
   "source": [
    "OpenHowNet.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4499e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HowNetDict' object has no attribute 'en_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9469/301337938.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhownet_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"苹果\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/programming/env/lib/python3.10/site-packages/OpenHowNet/HowNetDict.py\u001b[0m in \u001b[0;36mget_sense\u001b[0;34m(self, word, language, pos, strict)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzh_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programming/env/lib/python3.10/site-packages/OpenHowNet/HowNetDict.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[1;32m    108\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzh_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HowNetDict' object has no attribute 'en_map'"
     ]
    }
   ],
   "source": [
    "result_list = hownet_dict.get_sense(\"苹果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f26dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sleepy', 'worth', 'daisies', 'close', 'stupid', 'her', 'so', 'could', 'making', 'white', 'day', 'rabbit', 'eyes', 'considering', 'feel', 'trouble', 'hot', 'pleasure', 'ran', 'pink', 'daisychain', 'as', 'picking', 'getting', 'suddenly', 'mind'}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\"\n",
    "doc = nlp(sentence)\n",
    "nlp_sentence = list(doc.sents)[0]\n",
    "nlp_list_sent = []\n",
    "for word in nlp_sentence:\n",
    "    nlp_list_sent.append(word.lemma_)\n",
    "# print(nlp_list_sent)\n",
    "\n",
    "filtered_sent = \" \".join(nlp_list_sent)\n",
    "filtered_sent = remove_stopwords(sentence)\n",
    "filtered_sent = filtered_sent.translate(str.maketrans('', '', string.punctuation))\n",
    "filtered_sent = filtered_sent.split(' ')\n",
    "filtered_sent = [i.lower() for i in filtered_sent]\n",
    "filtered_sent = set(filtered_sent)\n",
    "# for word in filtered_sent:\n",
    "#     lemma_sent.append()\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34141f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to ache']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CEDICT[\"酸痛\"].meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37b754c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considering consider\n",
      "made make\n",
      "feel feel\n",
      "making make\n",
      "getting get\n",
      "picking pick\n",
      "ran run\n"
     ]
    }
   ],
   "source": [
    "for word in nlp_sentence:\n",
    "    if word.pos_ == \"VERB\":\n",
    "        print(word, word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc83345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
      "So considering mind (as could, hot day feel sleepy stupid), pleasure making daisy-chain worth trouble getting picking daisies, suddenly White Rabbit pink eyes ran close her.\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(remove_stopwords(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d0a8053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So',\n",
       " 'she',\n",
       " 'was',\n",
       " 'considering',\n",
       " 'in',\n",
       " 'her',\n",
       " 'own',\n",
       " 'mind',\n",
       " '(as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she',\n",
       " 'could,',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hot',\n",
       " 'day',\n",
       " 'made',\n",
       " 'her',\n",
       " 'feel',\n",
       " 'very',\n",
       " 'sleepy',\n",
       " 'and',\n",
       " 'stupid),',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'pleasure',\n",
       " 'of',\n",
       " 'making',\n",
       " 'a',\n",
       " 'daisy-chain',\n",
       " 'would',\n",
       " 'be',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'trouble',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'up',\n",
       " 'and',\n",
       " 'picking',\n",
       " 'the',\n",
       " 'daisies,',\n",
       " 'when',\n",
       " 'suddenly',\n",
       " 'a',\n",
       " 'White',\n",
       " 'Rabbit',\n",
       " 'with',\n",
       " 'pink',\n",
       " 'eyes',\n",
       " 'ran',\n",
       " 'close',\n",
       " 'by',\n",
       " 'her.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed6e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
